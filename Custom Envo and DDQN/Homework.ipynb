{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from gym.spaces import Discrete, Box \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Flatten, MaxPooling2D, Add\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "\n",
    "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
    "\n",
    "- This homework should be submitted in pairs.\n",
    "\n",
    "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
    "\n",
    "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
    "\n",
    "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
    "\n",
    "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
    "\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
    "```\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
    "\n",
    "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "## **PART 1 [35 points]: Custom Environment**\n",
    "<br />    \n",
    "\n",
    "In the first part of the homework, you are expected to define a custom environment. Based on the definition, you will implement policy iteration to compare different policies.\n",
    "    \n",
    "    \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "## **PART 1: Questions**\n",
    "<br />\n",
    "\n",
    "**ENVIRONMENT DESCRIPTION:**\n",
    "\n",
    "We have a hallway consisting of 5 blocks (states 0-4). There are two actions, which deterministically move the agent to the left or the right. More explicitly: Performing action “left” in state 0 keeps you in state 0, moves you from state 1 to state 0, from state 2 to state 1, state 3 to state 2, and state 4 to state 3. Performing action “right” in state 4 keeps you in state 4, moves you from state 3 to state 4, from state 2 to state 3, from state 1 to state 2, and from state 0 to state 1. The agent receives a reward of -1.0, if any action causes the agent to move to or remain at state 0, state 1, state 2, or state 3. The agent receives a reward of +10.0 if it goes to state 4. If the agent remains in state 4 for 3 consequtive steps, end the episode. Let the discount factor, γ = 0.75.\n",
    "\n",
    "Set the default initial state of the environment to be 0.\n",
    "    \n",
    "<img src=\"image/grid.png\" alt=\"Model Input\" style=\"width:500px\">\n",
    "<br /><br />\n",
    "\n",
    "### **1.1 [20 points] ENVIRONMENT DEFINITION**\n",
    "<br />\n",
    "\n",
    "**1.1.1** - Based on the environment definition, implement the custom environment with appropriate states, rewards and state transitions. Name this environment as `HallwayEnvironment`.\n",
    "<br /><br />\n",
    "\n",
    "**1.1.2** - The probability of moving in the direction as specified by the selected action is given 0.75 and the probability of moving in the other direction is 0.25. Define a probability transition matrix that gives the probability of going to one state from the given state. \n",
    "<br /><br />\n",
    "\n",
    "**1.1.3** - Define a reward matrix that gives the reward for being in a particular state. This is -1 for all states except the last one.\n",
    "<br /><br />\n",
    "\n",
    "### **1.2 [15 points] POLICY ITERATION**\n",
    "<br />\n",
    "\n",
    "**1.2.1**  Policy Evaluation: Initialize a policy “left” for every state. Implement policy evaluation as described in the lecture. That is, for each possible starting state, what is the expected sum of future rewards for this policy i.e. the value function? \n",
    "<br /><br />\n",
    "\n",
    "**1.2.2**  Policy Evaluation: Initialize a policy “random” for every state. Implement policy evaluation as described in the lecture. That is, for each possible starting state, what is the expected sum of future rewards for this policy i.e. the value function? \n",
    "<br /><br />\n",
    "\n",
    "**1.2.3** Provide a comparision of the 2 policies computing their value functions.\n",
    "<br /><br />\n",
    "    \n",
    "**1.2.4** Policy Iteration: Implement policy iteration. Report the sequence of policies you find starting with both the policies defined above in every state. Do they converge to the same policy? \n",
    "<br /><br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "## **PART 1: Solutions**\n",
    "<br />\n",
    "\n",
    "**ENVIRONMENT DESCRIPTION:**\n",
    "\n",
    "We have a hallway consisting of 5 blocks (states 0-4). There are two actions, which deterministically move the agent to the left or the right. More explicitly: Performing action “left” in state 0 keeps you in state 0, moves you from state 1 to state 0, from state 2 to state 1, state 3 to state 2, and state 4 to state 3. Performing action “right” in state 4 keeps you in state 4, moves you from state 3 to state 4, from state 2 to state 3, from state 1 to state 2, and from state 0 to state 1. The agent receives a reward of -1.0 if it starts any iteration in state 0, state 1, state 2, or state 3. The agent receives a reward of +10.0 if it starts in state 4. If the agent remains in state 4 for 3 consequtive steps, end the episode.  Let the discount factor γ = 0.75.\n",
    "\n",
    "Set the default initial state of the environment to be 0. However, this can change. The agent can choose to start in a different state.\n",
    "\n",
    "<img src=\"image/grid.png\" alt=\"Model Input\" style=\"width: 500px\">\n",
    "<br /><br />\n",
    "\n",
    "\n",
    "### **1.1 [20 points] ENVIRONMENT DEFINITION**\n",
    "<br />\n",
    "\n",
    "**1.1.1** - Based on the environment definition implement the custom environment with appropriate states, rewards and state transitions. Name this environment as `HallwayEnvironment`.\n",
    "<br />\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallwayEnvironment(Env):\n",
    "  \n",
    "  # The init function of the environment class\n",
    "  def __init__(self):\n",
    "\n",
    "    # The possible observation space is discrete 5 values\n",
    "    self.observation_space = Discrete(5)\n",
    "\n",
    "    # Possible actions are left and right\n",
    "    # Action 0 - LEFT\n",
    "    # Action 1 - RIGHT\n",
    "    self.action_space = Discrete(2)\n",
    "    \n",
    "    # The initial state of the agent in the environment\n",
    "    self.state = 0\n",
    "    \n",
    "    # Initial reward is 0\n",
    "    self.reward = 0\n",
    "\n",
    "    # Check if episode ended\n",
    "    self.done = 0\n",
    "    \n",
    "  def step(self, action):\n",
    "\n",
    "    # Set the action of the environment from the method parameter\n",
    "    self.action = action\n",
    "\n",
    "    # If action is left\n",
    "    if self.action==0:\n",
    "      if self.state==0:\n",
    "        next_state = self.state\n",
    "      elif self.state>0 and self.state<=4:\n",
    "        next_state = self.state - 1\n",
    "    \n",
    "    # If action is right\n",
    "    if self.action==1:\n",
    "      if self.state==4:\n",
    "        next_state = self.state\n",
    "      elif self.state<4 and self.state>=0:\n",
    "        next_state = self.state + 1\n",
    "\n",
    "    if self.state==4:\n",
    "      self.reward+=10\n",
    "      self.done+=1\n",
    "    else:\n",
    "      self.reward-=1\n",
    "      self.done=0\n",
    "\n",
    "    if self.done==3:\n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "\n",
    "    self.state = next_state\n",
    "\n",
    "    return self.state, self.reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "**1.1.2** - The probability of moving in the direction as specified by the selected action is given 0.75 and the probability of moving in the other direction is 0.25. Define a probability transition matrix that gives the probability of going to one state from the given state. \n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment\n",
    "env = HallwayEnvironment()\n",
    "\n",
    "# Get the number of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of states\n",
    "n_states = env.observation_space.n\n",
    "\n",
    "# The discount rate\n",
    "gamma = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the next state given the current state and action\n",
    "def get_state(state, action):\n",
    "\n",
    "  # If action is left\n",
    "  if action==0:\n",
    "    if state==0:\n",
    "      next_state = state\n",
    "    elif state>0 and state<=4:\n",
    "      next_state = state - 1\n",
    "  \n",
    "  # If action is right\n",
    "  if action==1:\n",
    "    if state==4:\n",
    "      next_state = state\n",
    "    elif state<4 and state>=0:\n",
    "      next_state = state + 1  \n",
    "\n",
    "  return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The action taken, the start state, the resultant state \n",
    "transition_probability = np.zeros(shape=(n_actions,n_states,n_states))\n",
    "\n",
    "# Loop over the number of actions\n",
    "for i in range(n_actions):\n",
    "\n",
    "  # Loop over the number of states\n",
    "  for j in range(n_states):\n",
    "    action = i\n",
    "    state = get_state(j, i)\n",
    "    transition_probability[action,j,state] = 0.75\n",
    "\n",
    "    # To set the probability in case of the opposite action\n",
    "    # The next state changes and hence we call the function again\n",
    "    if i==0:\n",
    "      action = 1\n",
    "      transition_probability[action,j,state] = 0.25\n",
    "    else:\n",
    "      action = 0\n",
    "      transition_probability[action,j,state] = 0.25\n",
    "\n",
    "print(\"STATE TRANSITION PROBABILITIES\\n\\n\", transition_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.1.3** - Define a reward matrix that gives the reward for being in a particular state. This is -1 for all states except the last one.\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = transition_probability\n",
    "\n",
    "reward_matrix = np.where(reward_matrix!=0, -1, reward_matrix)\n",
    "\n",
    "for i in range(2):\n",
    "  reward_matrix[i][3:5][0][4]=10\n",
    "  reward_matrix[i][3:5][1][4]=10\n",
    "\n",
    "reward_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "### **1.2 [15 points] POLICY ITERATION**\n",
    "<br />\n",
    "\n",
    "**1.2.1**  Policy Evaluation: Initialize a policy “left” for every state. Implement policy evaluation as described in the lecture. Get for each possible starting state, what is the expected sum of future rewards for this policy? \n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a left policy.\n",
    "left_policy = np.zeros(shape=(n_states, n_actions), dtype=int)\n",
    "left_policy[:,0] = 1\n",
    "left_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the value function\n",
    "# This function takes as input the policy and discount factor\n",
    "\n",
    "def policy_evaluation(policy, gamma):\n",
    "    \n",
    "    # Initialize the value table with zeros\n",
    "    # The size of the value table is the equal to the number of states\n",
    "    value_table = np.zeros(n_states)\n",
    "    \n",
    "    # Loop for a few iterations\n",
    "    for l in range(75):\n",
    "        \n",
    "        # Save the value table to the updated_value_table\n",
    "        updated_value_table = value_table\n",
    "\n",
    "        # For each state in the environment, \n",
    "        # select the action according to the policy and compute the value table\n",
    "        for state in range(n_states):\n",
    "\n",
    "            # Get the action from the policy based on the state\n",
    "            action = np.argmax(policy[state])\n",
    "            \n",
    "            # For the selected action, compute the value function\n",
    "            value_table[state] = np.sum([transition_probability[action][state][i]* ( reward_matrix[action][state][i] + gamma * updated_value_table[get_state(state, action)]) for i in range(len(transition_probability[action][state]))])\n",
    "\n",
    "    # Return the value table\n",
    "    return value_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = policy_evaluation(left_policy, gamma)\n",
    "print(\"Value Function of Left Policy\\n\\n\", value_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.2.2**  Policy Evaluation: Initialize a policy “random” for every state. Implement policy evaluation as described in the lecture. That is, for each possible starting state, what is the expected sum of future rewards for this policy i.e. the value function? \n",
    "\n",
    "<br /><br />\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a random policy.\n",
    "random_policy = np.zeros(shape=(n_states, n_actions), dtype=int)\n",
    "for i in range(len(random_policy)):\n",
    "    random_policy[i,0] = np.random.choice([0,1])\n",
    "    random_policy[i,1] = not random_policy[i,0]\n",
    "\n",
    "random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = policy_evaluation(random_policy, gamma)\n",
    "print(\"Value Function of Random Policy\\n\\n\", value_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "**1.2.3** Policy Iteration: Implement policy iteration. Report the sequence of policies you find starting with both the policies defined above in every state. Do they converge to the same policy? \n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the optimal policy based on the optimal value function\n",
    "# The function takes as input the value table and the discount factor\n",
    "\n",
    "def policy_improvement(value_table, policy, gamma = 1.0):\n",
    " \n",
    "    # Initialize the policy with zeros\n",
    "    # The size of the policy is equivalent to the number of states\n",
    "    policy = policy \n",
    "    \n",
    "    # Loop for each state \n",
    "    for state in range(n_states):\n",
    "        \n",
    "        # Initialize the Q table for a state\n",
    "        # with zeroes and the number of possible actions\n",
    "        Q_table = np.zeros(n_actions)\n",
    "        \n",
    "        # Loop for each action\n",
    "        for action in range(n_actions):\n",
    "            \n",
    "            # For the given state and action, loop over the next states\n",
    "            for i in range(len(transition_probability[action][state])): \n",
    "                \n",
    "                # Get the transition probability, next state, reward from the the environment\n",
    "                next_state = get_state(state, action)\n",
    "                trans_prob = transition_probability[action][state][i]\n",
    "                reward = reward_matrix[action][state][i]\n",
    "                \n",
    "                # Update the q-table the particular action based on the equation in the instructions\n",
    "                Q_table[action] += (trans_prob * (reward + gamma * value_table[next_state]))\n",
    "        \n",
    "        # Select the action which has maximum Q value as an optimal action of the state\n",
    "        policy[state][np.argmax(Q_table)] = 1\n",
    "        policy[state][np.argmax(Q_table) ^ 1] = 0\n",
    "    \n",
    "    # Return the policy\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform policy iteration\n",
    "# This function takes the environment and discount factor\n",
    "\n",
    "def policy_iteration(policy ,gamma = 1.0):\n",
    "    \n",
    "    # Initialize policy with zeros for the dimension of the number of state\n",
    "    old_policy = policy\n",
    "    \n",
    "    # Specify the number of iterations\n",
    "    no_of_iterations = 5\n",
    "    \n",
    "    # Loop over the number of iterations\n",
    "    for i in range(no_of_iterations):\n",
    "        \n",
    "        # Compute the value function calling the policy_evaluation \n",
    "        # by passing the policy and discount factor\n",
    "        new_value_function = policy_evaluation(old_policy, gamma)\n",
    "        \n",
    "        # Extract the new policy by calling the policy_improvement function with the \n",
    "        # new value function and the discount factor\n",
    "        new_policy = policy_improvement(new_value_function, old_policy, gamma)\n",
    "   \n",
    "        # Check whether we have reached convergence i.e whether we found the optimal\n",
    "        # policy by comparing old_policy and new policy.\n",
    "        # If the policies are the same, break the loop, else update the old policy\n",
    "        old_policy = new_policy\n",
    "        \n",
    "    return new_policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy_iteration(left_policy, gamma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy_iteration(random_policy, gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "## **PART 2 [65 points]: Deep Q Learning**\n",
    "<br />    \n",
    "\n",
    "In this part of the homework, you will work with the Breakout-v0 environment from OpenAI Gym. You will learn to play the game using a modified version of Deep Q Learning. \n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper code to get the ROMS file to work with Atari Environments directly on colab\n",
    "!gdown --id 1HhnGSXKgivZN4wGzrZCnjKAUAOJ18QuA\n",
    "!unzip Roms.zip\n",
    "!unzip -qq Roms/ROMS.zip\n",
    "!unzip -qq Roms/HC\\ ROMS.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms ROMS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "## **PART 2: Questions**\n",
    "<br />\n",
    "\n",
    "### **2.1 [10 points] PRE-PROCESSING**\n",
    "<br />\n",
    "\n",
    "Load the `Breakout-v0` environment from OpenAI Gym. Take a look at the possible actions and the state of the environment. What does each action integer represent? Describe the state of the environment. What does it represent?\n",
    "\n",
    "<br />\n",
    "\n",
    "### **2.2 [55 points] DQN Network**\n",
    "<br />\n",
    "\n",
    "\n",
    "**2.2.1** Define a replay memory to store the experiences which we will use to train the model later on.\n",
    "<br /><br />\n",
    "\n",
    "**2.2.2** Define a policy to take an action in each step of the environment.\n",
    "<br /><br />\n",
    "\n",
    "**2.2.3** Implement a DQN network which takes as input the state of the environment and returns the Q-value associated with each action you can take in that state. The Q-value is a sum of the value function and an advantage function. \n",
    "\n",
    "The architecture of the network is given in the image below.\n",
    "<img src=\"image/hw.png\" alt=\"Model Input\" style=\"width:700px\">\n",
    "    \n",
    "<br /><br />\n",
    "\n",
    "**2.2.4** Train the model with a policy and target network as discussed in the lecture.\n",
    "<br /><br />\n",
    "\n",
    "**2.2.5** Test the network by allowing your agent in the environment for 2 episodes and displaying the reward at the end of each episode.\n",
    "<br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "## **PART 2: Solutions**\n",
    "<br />\n",
    "\n",
    "### **2.1 [10 points] PRE-PROCESSING**\n",
    "<br />\n",
    "\n",
    "Load the `Breakout-v0` environment from OpenAI Gym. Take a look at the possible actions and the state of the environment. What does each action integer represent?\n",
    "\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the gym environment\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# Get information about the action and state space\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape\n",
    "\n",
    "print(\"The state space is:\", n_states)\n",
    "print(\"The number of actions is:\", n_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = (210,160,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaning of each action in the environment\n",
    "env.env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "### **2.2 [55 points] DQN Network**\n",
    "<br />\n",
    "\n",
    "\n",
    "**2.2.1** Define a replay memory to store the experiences which we will use to train the model later on.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a namedtuple class Experience that will hold the state, action, next state and reward\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class ReplayMemory to add experience to our class instance and\n",
    "# sample memory \n",
    "class ReplayMemory():\n",
    "\n",
    "  # __init__ function to initialize the memory capacity\n",
    "  def __init__ (self, capacity):\n",
    "    self.memory_capacity = capacity\n",
    "    self.memory = []\n",
    "    self.push_count = 0\n",
    "\n",
    "  # Method to add memory instance\n",
    "  def push(self, experience):\n",
    "    if len(self.memory) < self.memory_capacity:\n",
    "      self.memory.append(experience)\n",
    "    else:\n",
    "      self.memory[self.push_count % self.capacity] = experience\n",
    "    self.push_count+=1\n",
    "\n",
    "  # Method to sample from memory\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**2.2.2** Define a policy to take an action in each step of the environment.\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function epsilon greedy that takes as parameter, the neural network,\n",
    "# state and episode number\n",
    "def epsilon_greedy(policy_network, state, episode):\n",
    "\n",
    "  # Compute a random threshold value\n",
    "  threshold = np.random.uniform(0,1)\n",
    "\n",
    "  # Compute the epsilon value (exploration rate) based on the equation in the isntructions\n",
    "  epsilon = eps_min + (eps_max-eps_min) * np.exp(-eps_decay_rate*episode)\n",
    "\n",
    "  # Check if the threshold value is lower than the updated epsilon\n",
    "  if threshold < epsilon:\n",
    "\n",
    "      # Take a random action\n",
    "      action = env.action_space.sample()\n",
    "\n",
    "  # Else take the best action for that state by selecting the one that gives the\n",
    "  # highest q-value when given as input to the neural network\n",
    "  else:\n",
    "      action = np.argmax(state)\n",
    "\n",
    "  # Return the action\n",
    "  return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**2.2.3** Implement a DQN network which takes as input the state of the environment and returns the Q-value associated with each action you can take in that state. The Q-value is a sum of the value function and an advantage function. \n",
    "    \n",
    "The network architecture is given in the following figure.\n",
    "<img src=\"image/hw.png\" alt=\"Model Input\" style=\"width:700px\">\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the convolutional neural network for DQN\n",
    "def get_network(name):\n",
    "\n",
    "    input_shape = (n_states)\n",
    "\n",
    "    # Define size of the pooling operation\n",
    "    pool_size=(2,2)\n",
    "\n",
    "    inp = Input(shape=n_states, name=\"Input\")\n",
    "    l1 = Conv2D(256, kernel_size=(3,3), activation='relu', name=\"Conv1\")(inp)\n",
    "    l2 = Conv2D(128, kernel_size=(3,3), activation='relu', name=\"Conv2\")(l1)\n",
    "    m1 = MaxPooling2D(pool_size=pool_size, name=\"Pool1\")(l2)\n",
    "    l3 = Conv2D(64, kernel_size=(3,3), activation='relu', name=\"Conv3\")(m1)\n",
    "    m2 = MaxPooling2D(pool_size=pool_size, name=\"Pool2\")(l3)\n",
    "    l4 = Conv2D(32, kernel_size=(3,3), activation='relu', name=\"Conv4\")(m2)\n",
    "    m2 = MaxPooling2D(pool_size=pool_size, name=\"Pool3\")(l4)\n",
    "    flat = Flatten()(m2)\n",
    "\n",
    "    # Code for value function computation\n",
    "    d1 = Dense(256, activation='relu', name=\"Dense1\")(flat)\n",
    "    d2 = Dense(1, activation='linear', name=\"Dense2\")(d1)\n",
    "\n",
    "    # Code for advantage computation\n",
    "    d3 = Dense(256, activation='relu', name=\"Dense3\")(flat)\n",
    "    d4 = Dense(n_actions, activation='linear', name=\"Dense4\")(d3)\n",
    "\n",
    "    y = Add(name=\"Output\")([d2, d4])\n",
    "\n",
    "    model = Model(inputs=inp, outputs=y, name=name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**2.2.4** Train the model with a policy and target network as discussed in the lecture.\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the policy and target networks\n",
    "policy_network = get_network(\"Policy\")\n",
    "\n",
    "target_network = get_network(\"Target\")\n",
    "\n",
    "target_network.set_weights(policy_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Set the discount rate\n",
    "gamma = 0.99\n",
    "\n",
    "# Set the minimum discount rate\n",
    "eps_min = 0.01\n",
    "\n",
    "# Set the maximum discount rate\n",
    "eps_max = 1\n",
    "\n",
    "# Set the epsilon decay rate\n",
    "eps_decay_rate = 0.001\n",
    "\n",
    "# Set the number of updates to the policy network after\n",
    "# which we update the target network\n",
    "target_update = 100\n",
    "\n",
    "# Set capacity of replay memory\n",
    "memory_size = 210*160*500\n",
    "\n",
    "# Set learning rate of policy network\n",
    "lr = 0.001\n",
    "\n",
    "# Set the number of episodes\n",
    "num_episodes = 5\n",
    "\n",
    "# Warm-up steps - the number of experiences to store before\n",
    "# model training begins\n",
    "# It has to be atleast one more than the batch size\n",
    "warm_up_steps = batch_size*2\n",
    "\n",
    "# Set the number of steps in each episode\n",
    "steps_per_episode = 100\n",
    "\n",
    "# Variable to store the total number of steps in all episodes\n",
    "global_step_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ReplayMemory class by specifying the \n",
    "# memory capacity\n",
    "memory = ReplayMemory(memory_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer to train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Define the loss function of the model\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loop over all the episodes\n",
    "for episode in range(1,num_episodes+1):\n",
    "\n",
    "  # Print the spideo number\n",
    "  print(\"Episode:\", episode)\n",
    "\n",
    "  # Get the initial state of the environment  \n",
    "  state = env.reset()\n",
    "  state = state.mean(axis=2)\n",
    "\n",
    "  # Initialize a counter to update the target network\n",
    "  update_count = 0\n",
    "\n",
    "  # Loop over the maximium number of steps in the environment\n",
    "  for step in range(steps_per_episode):\n",
    "\n",
    "    # Increment the total step count variable\n",
    "    global_step_count+=1\n",
    "\n",
    "    # Get the action based on the epsilon greedy policy\n",
    "    action = epsilon_greedy(policy_network, state, episode)\n",
    "\n",
    "    # Get the next state, reward, whether or not the episode is done \n",
    "    # by calling the step method with the action\n",
    "    if action not in range(4):\n",
    "      action = np.random.choice(range(4))\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = next_state.mean(axis=2)\n",
    "\n",
    "    # Push this experience to the replay memory by calling the push method \n",
    "    # of the ReplayMemory class with the appropriate parameters\n",
    "    memory.push(Experience(state, action, next_state, reward))\n",
    "\n",
    "    # Update the current state of the environment\n",
    "    state = next_state\n",
    "\n",
    "    # Check if the total number of steps is greater than the warm-up steps to begin training\n",
    "    if global_step_count > warm_up_steps:\n",
    "\n",
    "      # Increment the counter to update the target network\n",
    "      update_count+=1\n",
    "\n",
    "      # Sample experiences from the memory\n",
    "      experiences = np.asarray(memory.sample(batch_size))\n",
    "\n",
    "      # Split the experience into a list of states, actions, next_states and rewards\n",
    "      states, actions, next_states, rewards = experiences[:,0].tolist(), experiences[:,1].tolist(), experiences[:,2].tolist(), experiences[:,3].tolist()\n",
    "\n",
    "      # Compute the target q-values from the target network by passing the next states \n",
    "      next_q_values = [tf.cast(tf.math.reduce_max(tf.cast(tf.math.reduce_max(target_network(np.array(next_states[i]).reshape((-1,)+n_states))), dtype=tf.float32)), dtype=tf.float32) for i in range(batch_size)]\n",
    "\n",
    "      target_q_values = [(next_q_values[i]*gamma)+ rewards[i] for i in range(batch_size)] \n",
    "\n",
    "      # Initialize a gradient tape\n",
    "      with tf.GradientTape() as tape:\n",
    "\n",
    "        # Get the predicted q-values from the policy network by passing the current states and the specific action from memory\n",
    "        predicted_q_values = [tf.cast(tf.math.reduce_max(policy_network(np.array(states).reshape((-1,)+n_states))), dtype=tf.float32)[int(actions[i])] for i in range(batch_size)]\n",
    "        \n",
    "        # Compute the loss between the target q-values and the predicted q-values\n",
    "        loss = loss_fn(target_q_values, predicted_q_values)\n",
    "\n",
    "      # Get the gradients wrt the policy network\n",
    "      gradients = tape.gradient(loss, policy_network.trainable_weights)\n",
    "\n",
    "      # Update the weights of the policy network\n",
    "      optimizer.apply_gradients(zip(gradients, policy_network.trainable_weights))\n",
    "    \n",
    "    # If the update counter has reached the update threshold defined as target_update above\n",
    "    if update_count%target_update==0:\n",
    "\n",
    "\n",
    "      # Update the weights of the target network with that of the policy network\n",
    "      target_network.set_weights(policy_network.get_weights())\n",
    "\n",
    "    # If done is True, then end the episode\n",
    "    if done==True:\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**2.2.5** Test the network by allowing your agent in the environment for 2 episodes and displaying the reward at the end of each episode.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the initial state of the environemnt\n",
    "state = env.reset()\n",
    "\n",
    "# Set done as False\n",
    "done = False\n",
    "\n",
    "reward = 0\n",
    "\n",
    "# Initialize a counter variable to keep track of the number of steps\n",
    "# before the agent reached the goal or falls into a hole\n",
    "count = 0\n",
    "\n",
    "# Loop over the episode\n",
    "while done!=True:\n",
    "    state = state.sum(axis=2)\n",
    "    # Get the action for the given state from the policy network\n",
    "    action = np.argmax(policy_network(np.array(state).reshape((-1,)+n_states)))\n",
    "    print(action)\n",
    "\n",
    "    # Get the next state and reward from the environment for the particular action taken\n",
    "    next_state, r, done, info = env.step(action) \n",
    "\n",
    "    # Print the state and action taken\n",
    "    print(\"State:\", state, \"\\tAction:\", action)\n",
    "\n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "\n",
    "    # Increment the step counter\n",
    "    count+=1\n",
    "\n",
    "    reward+=r\n",
    "\n",
    "print(\"Game ended after:\", count,\" steps with a reward of\", reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
