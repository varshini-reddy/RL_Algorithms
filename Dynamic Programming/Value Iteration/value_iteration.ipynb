{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import gym\n",
                "import numpy as np\n",
                "from helper import test_policy\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initializing an environment using a pre-defined environment from OpenAI Gym \n",
                "# The environment used here is 'FrozenLake-v0'\n",
                "env = gym.make('FrozenLake-v0')\n",
                "\n",
                "# Setting the initial parameters required for value iteration\n",
                "\n",
                "# Set the discount factor to a value between 0 and 1\n",
                "gamma = 0.99\n",
                "\n",
                "# Theta indicates the threshold determining the accuracy of the iteration\n",
                "# Set it to a value lower than 1e-3\n",
                "theta = 0.000001\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ How does theta affect the policy evaluation and value iteration algorithms?\n",
                "\n",
                "#### A. A large theta would cause the random policy to converge to the optimal policy much faster. \n",
                "#### B. Theta does not directly or indirectly affect finding the optimal policy.\n",
                "#### C. A large theta would cause policy evaluation to fasten but would slow down value iteration.\n",
                "#### D. A large theta would result in an optimal policy far from the true optimal policy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer1 = 'D'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **POLICY IMPROVEMENT**\n",
                "\n",
                "\u003cimg src=\"./images/policy_improvement.png\" alt=\"Policy Improvement\" style=\"width:700px\"\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function that returns the action which takes us to a higher valued state\n",
                "# It takes as input the environment, state-value function, policy, \n",
                "# action, current state and the discount rate\n",
                "def policy_improvement(env, V, pi, action, s, gamma):\n",
                "\n",
                "    # Initialize a numpy array of zeros with the same of the environment's action space\n",
                "    action_temp = np.zeros(env.action_space.n)\n",
                "\n",
                "    # Loop over the size of the environment's action space i.e.\n",
                "    # Iterate for every possible action\n",
                "    for a in range(env.action_space.n):                         \n",
                "\n",
                "        # Set the q value to 0\n",
                "        q = 0\n",
                "\n",
                "        # From the environment get P(s|a)\n",
                "        # This will return the probability of reaching successor state (s’) and its reward (r)\n",
                "        P = np.array(env.env.P[s][a])         \n",
                "        \n",
                "        # Iterate over the possible states\n",
                "        for i in range(len(P)):                              \n",
                "\n",
                "            # Get the possible succesor state\n",
                "            s_= int(P[i][1])                            \n",
                "\n",
                "            # Get the transition Probability P(s'|s,a) \n",
                "            p = P[i][0]                                 \n",
                "            \n",
                "            # Get the reward\n",
                "            r = P[i][2]                                 \n",
                "            \n",
                "            # Compute the action value q(s|a) based on the equation provided in the instruction\n",
                "            q += p*(r+gamma*V[s_])                      \n",
                "\n",
                "            # Save the q-value of taking a particular action into the action_temp array \n",
                "            action_temp[a] = q\n",
                "            \n",
                "    # Get the action that has the highest q-value \n",
                "    m = np.argmax(action_temp) \n",
                "\n",
                "    # For each state set the action which give the highest q-value\n",
                "    action[s] = m                                           \n",
                "    \n",
                "    # Update the policy by setting the action which give the highest q-value for a state as 1\n",
                "    pi[s][m] = 1                                        \n",
                "\n",
                "    # Return the updated policy\n",
                "    return pi\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **POLICY EVALUATION**\n",
                "\n",
                "\u003cimg src=\"./images/policy_eval.png\" alt=\"Policy Evaluation\" style=\"width:700px\"\u003e\n",
                "\n",
                "---\n",
                "___\n",
                "NOTE - We are not computing the max here as we already have the optimal policy. Instead, we just multiply with the policy which returns the value function of the best action (making others zero).\n",
                "\n",
                "---\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function to update the state value by taking the environment,\n",
                "# current state value, current state, policy and the discount rate\n",
                "def policy_evaluation(env, V, s, gamma):                           \n",
                "\n",
                "    # Initialize a policy as a matrix of zeros with size\n",
                "    # ( Number of state * Number of actions)\n",
                "    pi = np.zeros((env.observation_space.n, env.action_space.n))   \n",
                "        \n",
                "    # Set the initial value of all actions as zero\n",
                "    action = np.zeros((env.observation_space.n)) \n",
                "\n",
                "    # Initialize a numpy array of zeros with the same of the action space\n",
                "    action_temp = np.zeros(env.action_space.n) \n",
                "\n",
                "    # Call the policy_improvement function to get the policy\n",
                "    pi = policy_improvement(env, V, pi, action, s, gamma)                      \n",
                "    \n",
                "    # Set the initial value as 0\n",
                "    value = 0\n",
                "\n",
                "    # Loop over all possible actions\n",
                "    for a in range(env.action_space.n):\n",
                "\n",
                "        # Set u as 0 to compute the value of each state given \n",
                "        # the policy\n",
                "        u = 0\n",
                "\n",
                "        # From the environment get P(s|a)\n",
                "        P = np.array(env.env.P[s][a])\n",
                "\n",
                "        # Iterate over the possible states\n",
                "        for i in range(len(P)):   \n",
                "            \n",
                "            # Get the next state\n",
                "            s_= int(P[i][1])\n",
                "\n",
                "            # Get the probability of the next state given the current state\n",
                "            p = P[i][0]\n",
                "\n",
                "            # Get the reward of going from current state to next state\n",
                "            r = P[i][2]\n",
                "            \n",
                "            # Update the value function based on the equation provided in the instructions\n",
                "            u += p*(r+gamma*V[s_])\n",
                "            \n",
                "        # Update the value based on the policy and the value function\n",
                "        # This step is instead of the max defined by the image above\n",
                "        # Since we have the optimal policy, we just multiply the policy pi\n",
                "        # to get the value associated to the best action and the others become 0        value += pi[s,a] * u\n",
                "        value += pi[s,a] * u\n",
                "        \n",
                "    # Set the value function of the state as the value computed above\n",
                "    V[s]=value\n",
                "\n",
                "    # Return the value function\n",
                "    return V[s]\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ⏸ What does env.env.P[s][a] return?\n",
                "\n",
                "#### A. Probability of reaching successor state, successor state and reward.\n",
                "#### B. A list of all possible states that can be reached from s. \n",
                "#### C. Probability of reaching successor state, successor state, reward and whether the episode is done or not.\n",
                "#### D. A list of all possible states that can be reached from s on taking action a."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer2 = 'C'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **VALUE ITERATION** - Bringing everything together"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the function to perform value iteration\n",
                "def value_iteration(env, gamma, theta):\n",
                "\n",
                "    # Set the initial value of all states as zero\n",
                "    V = np.zeros(env.observation_space.n)      \n",
                "\n",
                "    # Initialize a loop\n",
                "    while True:\n",
                "\n",
                "        # Set delta as 0 to compare the estimation accuracy\n",
                "        delta = 0\n",
                "\n",
                "        # Loop over all the states\n",
                "        for s in range(env.observation_space.n):       \n",
                "\n",
                "            # Set the value as the state value function initialize above          \n",
                "            v = V[s]\n",
                "\n",
                "            # Update the state value function by calling the policy_evaluation function\n",
                "            V[s]= policy_evaluation(env, V, s, gamma)   \n",
                "            \n",
                "            # Compute the delta based on the changed in value per iteration using the equation\n",
                "            # given in the instructions\n",
                "            delta = max(delta, abs(v - V[s]))            \n",
                "        \n",
                "        # Check if the change is higher or lower than theta defined at the top\n",
                "        # If so then the value has converged to the optimal value\n",
                "        if delta \u003c theta:                                       \n",
                "            break           \n",
                "\n",
                "\n",
                "    # # Initialize a policy as a matrix of zeros with size\n",
                "    # # ( Number of state * Number of actions)\n",
                "    pi = np.zeros((env.observation_space.n, env.action_space.n)) \n",
                "\n",
                "    # Set the initial value of all actions as zero\n",
                "    action = np.zeros((env.observation_space.n))                              \n",
                "\n",
                "    # To extract the optimal policy loop over all the states\n",
                "    for s in range(env.observation_space.n):\n",
                "\n",
                "        # Call the policy_control function to get the optimal policy\n",
                "        pi = policy_improvement(env, V, pi, action, s, gamma)         \n",
                "\n",
                "    # Return the optimal value function, the policy and the action sequence\n",
                "    return V, pi,action      \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **TESTING THE POLICY**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "THE ACTION TO TAKE IN A GIVEN STATE IS:\n [[0. 3. 3. 3.]\n [0. 0. 0. 0.]\n [3. 1. 0. 0.]\n [0. 2. 1. 0.]]\n\n\n\nTHE OPTIMAL POLICY IS:\n [[1. 0. 0. 0.]\n [0. 0. 0. 1.]\n [0. 0. 0. 1.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [0. 0. 0. 1.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]]\n"
                }
            ],
            "source": [
                "# Call the value_iteration function to get the action sequence, \n",
                "# optimal policy and value function\n",
                "V, pi, action = value_iteration(env, gamma, theta)\n",
                "\n",
                "# Print the discrete action to take in a given state\n",
                "print(\"THE ACTION TO TAKE IN A GIVEN STATE IS:\\n\", np.reshape(action,(4,4)))                    \n",
                "\n",
                "# Print the optimal policy\n",
                "print(\"\\n\\n\\nTHE OPTIMAL POLICY IS:\\n\", pi)\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": " Agent succeeded to reach goal 0 out of 100 episodes using a random policy \n Agent succeeded to reach goal 76 out of 100 episodes using the optimal policy \n"
                }
            ],
            "source": [
                "# Use the helper function test_policy in the helper file to compute the \n",
                "# number of times the agent reaches the goal within a fixed number of steps \n",
                "# in each episode\n",
                "# Every time the agent reaches the goal within the fixed step we call it a sucsess\n",
                "\n",
                "# Set a variable random as 1\n",
                "# This will ensure that the test_policy function gives the result of some random policy\n",
                "random = 1\n",
                "\n",
                "# Call the test_policy function by passing the environment, action and random\n",
                "test_policy(env, action, random)\n",
                "\n",
                "# Set a variable random as 0\n",
                "# This will ensure that the test_policy function gives the result of the optimal policy\n",
                "random = 0\n",
                "\n",
                "# Call the test_policy function by passing the environment, action and random\n",
                "test_policy(env, action, random)"
            ]
        }
    ]
}
