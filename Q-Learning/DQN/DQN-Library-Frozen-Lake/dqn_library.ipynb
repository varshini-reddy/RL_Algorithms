{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessay libraries\n",
                "import numpy as np\n",
                "import gym\n",
                "\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Activation, Flatten\n",
                "from keras.optimizers import Adam\n",
                "\n",
                "from rl.agents.dqn import DQNAgent\n",
                "from rl.policy import EpsGreedyQPolicy\n",
                "from rl.memory import SequentialMemory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initializing an environment using a pre-defined environment from OpenAI Gym \n",
                "# The environment used here is 'FrozenLake-v0'\n",
                "env = gym.make(\"FrozenLake-v0\")\n",
                "\n",
                "# Get the number of actions within the environment\n",
                "nb_actions = env.action_space.n\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a Feed-Forward Neural Network \n",
                "\n",
                "# Initialize a keras sequential model\n",
                "model = Sequential()\n",
                "\n",
                "# Flatten the input to have an input shape of (1,) + shape of the environment state space\n",
                "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
                "\n",
                "# Add Dense layers with Relu activation\n",
                "# The number of hidden layers and number of nodes in each layer is your choice\n",
                "model.add(Dense(128))\n",
                "model.add(Activation('relu'))\n",
                "\n",
                "model.add(Dense(256))\n",
                "model.add(Activation('relu'))\n",
                "\n",
                "# Add an output layer with number of nodes as the number of actions\n",
                "model.add(Dense(nb_actions))\n",
                "model.add(Activation('linear'))\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the policy to sample the actions\n",
                "# We will be using the Epsilon-Greedy algorithm\n",
                "policy = EpsGreedyQPolicy()\n",
                "\n",
                "# To store our data initialize Sequential Memory with limit=500000 and window_length of 1\n",
                "memory = SequentialMemory(limit=500000, window_length=1)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **DQN AGENT**\n",
                "\n",
                "\u003cimg src=\"./images/dqn.png\" alt=\"DQN Agent\" style=\"width:700px\"\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
                }
            ],
            "source": [
                "\n",
                "# Initialize the DQNAgent with the neural network model, nb_actions as the number of actions in the environment, \n",
                "# set the memory as the sequential memory defined above, nb_steps_warmup as 100, policy as the epsilon greedy policy defined above\n",
                "# and set the target_model_update as 1e-2\n",
                "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100, target_model_update=1e-2, policy=policy)\n",
                "\n",
                "# Compile the DQN with Adam optimizer with learning rate of 1e-3 and metric as mse\n",
                "dqn.compile(Adam(lr=1e-3), metrics=['mse'])\n",
                "\n",
                "# Fit the DQN by passing with environment with nb_steps as 5000\n",
                "# You have an option to visualize the output, which is done by implicitly calling the render function of the environment\n",
                "# However, this will slow down the training process and is not recommended for EdStem\n",
                "# To see the complete training details, set verbose as 2\n",
                "dqn.fit(env, nb_steps=5000, visualize=False, verbose=0);\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Testing for 10 episodes ...\nEpisode 1: reward: 0.000, steps: 10\nEpisode 2: reward: 0.000, steps: 59\nEpisode 3: reward: 0.000, steps: 15\nEpisode 4: reward: 0.000, steps: 15\nEpisode 5: reward: 0.000, steps: 17\nEpisode 6: reward: 0.000, steps: 8\nEpisode 7: reward: 1.000, steps: 19\nEpisode 8: reward: 0.000, steps: 61\nEpisode 9: reward: 0.000, steps: 24\nEpisode 10: reward: 1.000, steps: 26\n"
                },
                {
                    "data": {
                        "text/plain": [
                            "\u003ctensorflow.python.keras.callbacks.History at 0x7f4903b70250\u003e"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Test your model by passing the environment and running for 10 episodes\n",
                "dqn.test(env, nb_episodes=10, visualize=False)\n",
                ""
            ]
        }
    ]
}
