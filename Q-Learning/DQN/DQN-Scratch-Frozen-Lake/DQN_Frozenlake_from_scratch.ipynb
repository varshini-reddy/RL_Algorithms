{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# IMPORT NECESSARY LIBRARIES\n",
                "\n",
                "import gym\n",
                "import math\n",
                "import random\n",
                "import numpy as np\n",
                "import matplotlib\n",
                "import tensorflow as tf\n",
                "from itertools import count\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import namedtuple\n",
                "from tensorflow.keras import Sequential, Model\n",
                "from tensorflow.keras.layers import Flatten, Conv2D, Dense, Input\n",
                "%matplotlib inline\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GET THE ENVIRONMENT\n",
                "\n",
                "# Initialize the FrozenLake-v0 gym environment\n",
                "env = gym.make(\"FrozenLake-v0\")\n",
                "\n",
                "# Get the number of possible actions\n",
                "n_outputs = env.action_space.n\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NEURAL NETWORK MODEL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to return the neural network model\n",
                "# The input to the network is the state of the environment\n",
                "# It should return the q-value for all possible actions\n",
                "\n",
                "def get_model(name):\n",
                "  model = Sequential(name=name)\n",
                "  model.add(Input(1, name=\"Input\"))\n",
                "  model.add(Dense(24, activation='relu', name=\"Hidden1\"))\n",
                "  model.add(Dense(32, activation='relu', name=\"Hidden2\"))\n",
                "  model.add(Dense(n_outputs, activation='linear', name=\"Output\"))\n",
                "  return model\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### REPLAY MEMORY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating a namedtuple class Experience that will hold the state, action, next state and reward\n",
                "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward'))\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class ReplayMemory to add experience to our class instance and\n",
                "# sample memory \n",
                "class ReplayMemory():\n",
                "\n",
                "  # __init__ function to initialize the memory capacity\n",
                "  def __init__ (self, capacity):\n",
                "    self.memory_capacity = capacity\n",
                "    self.memory = []\n",
                "    self.push_count = 0\n",
                "\n",
                "  # Method to add memory instance\n",
                "  def push(self, experience):\n",
                "    if len(self.memory) \u003c self.memory_capacity:\n",
                "      self.memory.append(experience)\n",
                "    else:\n",
                "      self.memory[self.push_count % self.capacity] = experience\n",
                "    self.push_count+=1\n",
                "\n",
                "  # Method to sample from memory\n",
                "  def sample(self, batch_size):\n",
                "    return random.sample(self.memory, batch_size)\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ACTION SELECTION - EPSILON GREEDY POLICY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function epsilon greedy that takes as parameter, the neural network,\n",
                "# state and episode number\n",
                "def epsilon_greedy(policy_network, state, episode):\n",
                "\n",
                "  # Compute a random threshold value\n",
                "  threshold = np.random.uniform(0,1)\n",
                "\n",
                "  # Compute the epsilon value (exploration rate) based on the equation in the isntructions\n",
                "  epsilon = eps_min + (eps_max-eps_min) * np.exp(-eps_decay_rate*episode)\n",
                "\n",
                "  # Check if the threshold value is lower than the updated epsilon\n",
                "  if threshold \u003c epsilon:\n",
                "\n",
                "      # Take a random action\n",
                "      action = env.action_space.sample()\n",
                "\n",
                "  # Else take the best action for that state by selecting the one that gives the\n",
                "  # highest q-value when given as input to the neural network\n",
                "  else:\n",
                "      q_value = tf.reshape(tf.convert_to_tensor(state), shape=(-1,1) )\n",
                "      action = np.argmax(policy_network(q_value))\n",
                "\n",
                "  # Return the action\n",
                "  return action\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ALGORITHM PARAMETERS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the batch size\n",
                "batch_size = 256\n",
                "\n",
                "# Set the discount rate\n",
                "gamma = 0.99\n",
                "\n",
                "# Set the minimum discount rate\n",
                "eps_min = 0.01\n",
                "\n",
                "# Set the maximum discount rate\n",
                "eps_max = 1\n",
                "\n",
                "# Set the epsilon decay rate\n",
                "eps_decay_rate = 0.001\n",
                "\n",
                "# Set the number of updates to the policy network after\n",
                "# which we update the target network\n",
                "target_update = 100\n",
                "\n",
                "# Set capacity of replay memory\n",
                "memory_size = 100000\n",
                "\n",
                "# Set learning rate of policy network\n",
                "lr = 0.001\n",
                "\n",
                "# Set the number of episodes\n",
                "num_episodes = 1000\n",
                "\n",
                "# Warm-up steps - the number of experiences to store before\n",
                "# model training begins\n",
                "# It has to be atleast one more than the batch size\n",
                "warm_up_steps = batch_size*2\n",
                "\n",
                "# Set the number of steps in each episode\n",
                "steps_per_episode = 100\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### THE DQN - TRAINING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an instance of the ReplayMemory class by specifying the \n",
                "# memory capacity\n",
                "memory = ReplayMemory(memory_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the policy network by calling the get_model function\n",
                "policy_network = get_model(name='Policy_Network')\n",
                "\n",
                "# Initialize the target network by calling the get_model function\n",
                "target_network = get_model(name='Target_Network')\n",
                "\n",
                "# Set the target network weights equal to that of the policy network\n",
                "target_network.set_weights(policy_network.get_weights())\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the optimizer to train the model\n",
                "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
                "\n",
                "# Define the loss function of the model\n",
                "loss_fn = tf.keras.losses.MeanSquaredError()\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variable to store the total number of steps in all episodes\n",
                "global_step_count = 0\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Target Policy Updated after epsiode 1\nTarget Policy Updated after epsiode 1\nTarget Policy Updated after epsiode 1\nTarget Policy Updated after epsiode 1\nTarget Policy Updated after epsiode 2\nTarget Policy Updated after epsiode 2\nTarget Policy Updated after epsiode 2\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 3\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 4\nTarget Policy Updated after epsiode 5\nTarget Policy Updated after epsiode 5\nTarget Policy Updated after epsiode 5\nTarget Policy Updated after epsiode 5\nTarget Policy Updated after epsiode 5\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 6\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 7\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 8\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 9\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 10\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 11\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 12\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 13\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 14\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 15\nTarget Policy Updated after epsiode 16\nTarget Policy Updated after epsiode 16\nTarget Policy Updated after epsiode 16\nTarget Policy Updated after epsiode 16\nTarget Policy Updated after epsiode 16\nTarget Policy Updated after epsiode 17\nTarget Policy Updated after epsiode 17\nTarget Policy Updated after epsiode 17\nTarget Policy Updated after epsiode 18\nTarget Policy Updated after epsiode 18\nTarget Policy Updated after epsiode 18\nTarget Policy Updated after epsiode 18\nTarget Policy Updated after epsiode 19\nTarget Policy Updated after epsiode 19\nTarget Policy Updated after epsiode 19\nTarget Policy Updated after epsiode 19\nTarget Policy Updated after epsiode 19\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 20\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 21\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 22\nTarget Policy Updated after epsiode 23\nTarget Policy Updated after epsiode 23\nTarget Policy Updated after epsiode 24\nTarget Policy Updated after epsiode 24\nTarget Policy Updated after epsiode 24\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 25\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 26\nTarget Policy Updated after epsiode 27\nTarget Policy Updated after epsiode 27\nTarget Policy Updated after epsiode 27\nTarget Policy Updated after epsiode 27\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 28\nTarget Policy Updated after epsiode 29\nTarget Policy Updated after epsiode 29\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 30\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 31\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 32\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 33\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 34\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 35\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 36\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 37\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 38\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 39\nTarget Policy Updated after epsiode 40\nTarget Policy Updated after epsiode 40\nTarget Policy Updated after epsiode 40\nTarget Policy Updated after epsiode 40\nTarget Policy Updated after epsiode 40\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 41\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 42\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 43\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 44\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 45\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 46\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 47\nTarget Policy Updated after epsiode 48\nTarget Policy Updated after epsiode 48\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 49\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 50\nTarget Policy Updated after epsiode 51\nTarget Policy Updated after epsiode 51\nTarget Policy Updated after epsiode 51\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 52\nTarget Policy Updated after epsiode 53\nTarget Policy Updated after epsiode 53\nTarget Policy Updated after epsiode 53\nTarget Policy Updated after epsiode 53\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 54\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 55\nTarget Policy Updated after epsiode 56\nTarget Policy Updated after epsiode 56\nTarget Policy Updated after epsiode 56\nTarget Policy Updated after epsiode 56\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 57\nTarget Policy Updated after epsiode 58\nTarget Policy Updated after epsiode 58\nTarget Policy Updated after epsiode 58\n"
                }
            ],
            "source": [
                "# Loop over all the episodes\n",
                "for episode in range(1,num_episodes+1):\n",
                "\n",
                "  # Get the initial state of the environment  \n",
                "  state = env.reset()\n",
                "\n",
                "  # Initialize a counter to update the target network\n",
                "  update_count = 0\n",
                "\n",
                "  # Loop over the maximium number of steps in the environment\n",
                "  for step in range(steps_per_episode):\n",
                "\n",
                "    # Increment the total step count variable\n",
                "    global_step_count+=1\n",
                "\n",
                "    # Get the action based on the epsilon greedy policy\n",
                "    action = epsilon_greedy(policy_network, state, episode)\n",
                "\n",
                "    # Get the next state, reward, whether or not the episode is done \n",
                "    # by calling the step method with the action\n",
                "    next_state, reward, done, info = env.step(action)\n",
                "\n",
                "    # Push this experience to the replay memory by calling the push method \n",
                "    # of the ReplayMemory class with the appropriate parameters\n",
                "    memory.push(Experience(state, action, next_state, reward))\n",
                "\n",
                "    # Update the current state of the environment\n",
                "    state = next_state\n",
                "\n",
                "    # Check if the total number of steps is greater than the warm-up steps to begin training\n",
                "    if global_step_count \u003e warm_up_steps:\n",
                "\n",
                "      # Increment the counter to update the target network\n",
                "      update_count+=1\n",
                "\n",
                "      # Sample experiences from the memory\n",
                "      experiences = np.asarray(memory.sample(batch_size))\n",
                "\n",
                "      # Split the experience into a list of states, actions, next_states and rewards\n",
                "      states, actions, next_states, rewards = experiences[:,0].tolist(), experiences[:,1].tolist(), experiences[:,2].tolist(), experiences[:,3].tolist()\n",
                "\n",
                "      # Compute the target q-values from the target network by passing the next states \n",
                "      next_q_values = [tf.cast(tf.math.reduce_max(target_network(tf.reshape(tf.convert_to_tensor(next_states[i]), shape=(-1,1)))), dtype=tf.float32) for i in range(batch_size)]\n",
                "      # next_q_values = [max(target_network(tf.reshape(tf.convert_to_tensor(next_states[i]), shape=(-1,1)))[0].numpy())  for i in range(batch_size)]\n",
                "      target_q_values = [(next_q_values[i]*gamma)+ rewards[i] for i in range(batch_size)] \n",
                "\n",
                "      # Initialize a gradient tape\n",
                "      with tf.GradientTape() as tape:\n",
                "\n",
                "        # Get the predicted q-values from the policy network by passing the current states and the specific action from memory\n",
                "        predicted_q_values = [policy_network(tf.reshape(tf.convert_to_tensor(states[i]), shape=(-1,1)))[0][int(actions[i])] for i in range(batch_size)]\n",
                "        \n",
                "        # Compute the loss between the target q-values and the predicted q-values\n",
                "        loss = loss_fn(target_q_values, predicted_q_values)\n",
                "\n",
                "      # Get the gradients wrt the policy network\n",
                "      gradients = tape.gradient(loss, policy_network.trainable_weights)\n",
                "\n",
                "      # Update the weights of the policy network\n",
                "      optimizer.apply_gradients(zip(gradients, policy_network.trainable_weights))\n",
                "    \n",
                "    # If the update counter has reached the update threshold defined as target_update above\n",
                "    if update_count%target_update==0:\n",
                "\n",
                "      # Update the weights of the target network with that of the policy network\n",
                "      print(\"Target Policy Updated after \", update_count, \"updates to the policy network\")\n",
                "      target_network.set_weights(policy_network.get_weights())\n",
                "\n",
                "    # If done is True, then end the episode\n",
                "    if done==True:\n",
                "      break\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PREDICTION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "State: 0 \tAction: 2\nState: 0 \tAction: 2\nState: 0 \tAction: 2\nState: 4 \tAction: 1\nState: 4 \tAction: 1\nOops, fell into the hole!\n\n  (Down)\nSFFF\nF\u001b[41mH\u001b[0mFH\nFFFH\nHFFG\n"
                }
            ],
            "source": [
                "# Get the initial state of the environemnt\n",
                "state = env.reset()\n",
                "\n",
                "# Set done as False\n",
                "done = False\n",
                "\n",
                "# Initialize a counter variable to keep track of the number of steps\n",
                "# before the agent reached the goal or falls into a hole\n",
                "count = 0\n",
                "\n",
                "# Loop over the episode\n",
                "while done!=True:\n",
                "\n",
                "  # Get the action for the given state from the policy network\n",
                "  action = np.argmax(policy_network(tf.reshape(tf.convert_to_tensor(state), shape=(-1,1))))\n",
                "\n",
                "  # Get the next state and reward from the environment for the particular action taken\n",
                "  next_state, reward, done, info = env.step(action) \n",
                "\n",
                "  # Print the state and action taken\n",
                "  print(\"State:\", state, \"\\tAction:\", action)\n",
                "\n",
                "  # Update the current state\n",
                "  state = next_state\n",
                "\n",
                "  # Increment the step counter\n",
                "  count+=1\n",
                "\n",
                "# Check if the agent has reached the goal\n",
                "if state ==15:\n",
                "  print(\"Goal reached after\",count, \"steps\\n\")\n",
                "else:\n",
                "  print(\"Oops, fell into the hole!\\n\")\n",
                "\n",
                "env.render()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
