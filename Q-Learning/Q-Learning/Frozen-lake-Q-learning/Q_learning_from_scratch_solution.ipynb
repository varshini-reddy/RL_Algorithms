{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### IMPORT NECESSARY LIBRARIES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import gym\n",
                "import time\n",
                "import random\n",
                "import numpy as np\n",
                "from IPython.display import clear_output"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### INTIALIZE THE ENVIRONMENT AND Q-TABLE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "INITIAL Q-TABLE\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
                }
            ],
            "source": [
                "# Initialize the FrozenLake-v0 gym environment\n",
                "env = gym.make(\"FrozenLake-v0\")\n",
                "\n",
                "# Get the size of the action and state space\n",
                "n_actions = env.action_space.n\n",
                "n_states = env.observation_space.n\n",
                "\n",
                "# Initialize the Q-table with zeroes with dimension states*actions\n",
                "q_table = np.zeros((n_states, n_actions))\n",
                "\n",
                "# Print the initial Q-table\n",
                "print(\"INITIAL Q-TABLE\")\n",
                "print(q_table)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SET THE ALGORITHM PARAMETERS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the exploration rate\n",
                "epsilon = 1\n",
                "\n",
                "# Set the minimum exploration rate\n",
                "eps_min = 0.01\n",
                "\n",
                "# Set the maximum exploration rate\n",
                "eps_max = 1.0\n",
                "\n",
                "# Set the epsilon decay rate\n",
                "# Set it to a low value for better results\n",
                "eps_decay_rate = 0.01\n",
                "\n",
                "# Set the learning rate to compute the q-value\n",
                "learning_rate = 0.1\n",
                "\n",
                "# Set the discount factor\n",
                "gamma = 0.99\n",
                "\n",
                "# Set the number of episodes\n",
                "num_episodes = 10000\n",
                "\n",
                "# Set the maximum number of steps for each episode\n",
                "num_steps_per_epi = 100\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### EPSILON GREEDY POLICY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function epsilon greedy that takes as parameter the q-table,\n",
                "# state and episode number\n",
                "def epsilon_greedy(q_table, state, episode):\n",
                "\n",
                "  # Compute a random threshold value\n",
                "  threshold = np.random.uniform(0,1)\n",
                "\n",
                "  # Compute the epsilon value (exploration rate) based on the equation in the isntructions\n",
                "  epsilon = eps_min + (eps_max-eps_min) * np.exp(-eps_decay_rate*episode)\n",
                "\n",
                "  # Check if the threshold value is lower than the updated epsilon\n",
                "  if threshold \u003c epsilon:\n",
                "\n",
                "      # Take a random action\n",
                "      action = env.action_space.sample()\n",
                "\n",
                "  # Else take the best action for that state based to the q-table\n",
                "  else:\n",
                "      action = np.argmax(q_table[state,:])\n",
                "\n",
                "  # Return the action\n",
                "  return action\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q-LEARNING ALGORITHM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List to store the rewards of each episode\n",
                "total_reward_episode = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loop over the number of episodes\n",
                "for episode in range(num_episodes):\n",
                "\n",
                "  # Set the initial state of the environment\n",
                "  state = env.reset()\n",
                "\n",
                "  # Set done as False\n",
                "  done = False\n",
                "\n",
                "  # Initialize the reward for this episode as 0\n",
                "  reward_episode = 0\n",
                "\n",
                "  # Loop over the maximum number of steps in each episode\n",
                "  for step in range(num_steps_per_epi):\n",
                "\n",
                "    # Get the action based on the epsilong greedy policy\n",
                "    action = epsilon_greedy(q_table, state, episode)\n",
                "\n",
                "    # Call the step method with the action to get the next state and reward \n",
                "    next_state, reward, done, info = env.step(action)\n",
                "\n",
                "    # Compute the q value based on the equation in the instructions\n",
                "    q_value = (q_table[state, action] * (1-learning_rate)) + (learning_rate * (reward + gamma*(q_table[next_state, action])))\n",
                "\n",
                "    # Update the q-table for the q-value\n",
                "    q_table[state, action] = q_value\n",
                "\n",
                "    # Update the state \n",
                "    state = next_state\n",
                "\n",
                "    # Update the reward for this episode\n",
                "    reward_episode+= reward\n",
                "\n",
                "    # End the loop for this episode if done is True\n",
                "    if done==True:\n",
                "      break\n",
                "\n",
                "  # Append the reward to the reward list \n",
                "  total_reward_episode.append(reward_episode)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 96,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Q-TABLE\n[[0.         0.04573663 0.0669407  0.        ]\n [0.         0.03642594 0.0670219  0.        ]\n [0.         0.04831736 0.10000783 0.        ]\n [0.         0.01502818 0.         0.        ]\n [0.         0.06022531 0.06610448 0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.08274409 0.1614667  0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.08646087 0.09637048 0.        ]\n [0.         0.19262956 0.30497255 0.        ]\n [0.         0.24276446 0.2628909  0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.24813806 0.47648134 0.19435937]\n [0.         0.57709871 0.7052317  0.48218848]\n [0.         0.         0.         0.        ]]\n"
                }
            ],
            "source": [
                "# Print the updated q-table\n",
                "print(\"Q-TABLE\")\n",
                "print(q_table)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TEST THE Q-TABLE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "EPISODE:  3 \n\n  (Right)\nSFFF\nF\u001b[41mH\u001b[0mFH\nFFFH\nHFFG\n\nFell in the hole!!\n"
                }
            ],
            "source": [
                "# Number of test episodes\n",
                "num_test_episode = 3\n",
                "\n",
                "# Loop over the number of test episodes\n",
                "for episode in range(num_test_episode):\n",
                "\n",
                "  # Get the initial state os the environment\n",
                "  state = env.reset()\n",
                "\n",
                "  # Set done as False\n",
                "  done = False\n",
                "\n",
                "  # Print episode number\n",
                "  print(\"EPISODE: \", episode+1,\"\\n\")\n",
                "  time.sleep(1)\n",
                "\n",
                "  # Loop over the maximum number of steps per episode\n",
                "  for step in range(num_steps_per_epi):\n",
                "\n",
                "    # Print the episode output\n",
                "    clear_output(wait=True)\n",
                "    env.render()\n",
                "    time.sleep(0.3)\n",
                "\n",
                "    # Get the next state based on the q-table\n",
                "    action = np.argmax(q_table[state,:])\n",
                "\n",
                "    # Get the next state given the action\n",
                "    next_state, reward, done, info = env.step(action)\n",
                "\n",
                "    # Helper code to plot the agent state and action in the environment \n",
                "    if done:\n",
                "      clear_output(wait=True)\n",
                "      env.render()\n",
                "      if reward==1:\n",
                "        print(\"\\nGoal Reached!\")\n",
                "        time.sleep(3)\n",
                "      else:\n",
                "        print(\"\\nFell in the hole!!\")\n",
                "        time.sleep(3)\n",
                "      clear_output(wait=True)\n",
                "      break\n",
                "  \n",
                "    # Update the state\n",
                "    state = next_state\n",
                "\n",
                "# Close the environment\n",
                "env.close()\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MAKE CHANGES\n",
                "\n",
                "If your q-table does not result in your agent reaching the goal, alter the algorithm parameters and try again."
            ]
        }
    ]
}
